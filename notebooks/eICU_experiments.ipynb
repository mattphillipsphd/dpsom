{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eICU Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:15.356772Z",
     "start_time": "2020-02-17T10:29:09.839675Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow Read-me instruction to download the medical data.\n",
    "\n",
    "After having downloaded the data in '../data/eICU_data.csv', upload the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:15.366621Z",
     "start_time": "2020-02-17T10:29:15.359625Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(test=True):\n",
    "#    hf = h5py.File('../data/eICU_data.csv', 'r')\n",
    "    hf = h5py.File('/home/matt/Datasets/EHRs/eICU/eICU_data_b72_nonorm.csv', 'r')\n",
    "    data_total = np.array(hf.get('x'))\n",
    "    endpoints_total = np.array(hf.get('y'))\n",
    "    hf.close()\n",
    "    data_train, data_val, y_train, endpoints_total_val = train_test_split(data_total[:int(len(data_total) * 0.85)],\n",
    "                                                                          endpoints_total[:int(len(data_total) * 0.85)],\n",
    "                                                                          test_size=0.20,\n",
    "                                                                          random_state=42)\n",
    "    if test:\n",
    "        data_val = data_total[int(len(data_total) * 0.85):]\n",
    "        endpoints_total_val = endpoints_total[int(len(data_total) * 0.85):]\n",
    "    return data_train, data_val, y_train, endpoints_total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:15.376274Z",
     "start_time": "2020-02-17T10:29:15.369561Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(data_train, data_val, endpoints_total_val, batch_size, mode=\"train\"):\n",
    "    while True:\n",
    "        if mode == \"train\":\n",
    "            for i in range(len(data_train) // batch_size):\n",
    "                time_series = data_train[i * batch_size: (i + 1) * batch_size]\n",
    "                yield time_series, i\n",
    "        elif mode == \"val\":\n",
    "            for i in range(len(data_val) // batch_size):\n",
    "                time_series = data_val[i * batch_size: (i + 1) * batch_size]\n",
    "                time_series_endpoint = endpoints_total_val[i * batch_size: (i + 1) * batch_size]\n",
    "                yield time_series, time_series_endpoint, i\n",
    "        else:\n",
    "            raise ValueError(\"The mode has to be in {train, val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the name of the job in ex_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:15.383035Z",
     "start_time": "2020-02-17T10:29:15.379004Z"
    }
   },
   "outputs": [],
   "source": [
    "ex_name=\"hyperopt_LSTM_20_16-16_2020-02-17_35a17\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:16.663266Z",
     "start_time": "2020-02-17T10:29:15.385146Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "modelpath = \"/home/matt/Training/dpsom/models/{}/{}\".format(ex_name, ex_name)\n",
    "data_train, data_val, endpoints_total_train, endpoints_total_val = get_data(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create heat-maps, trajectories and probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:31:01.048797Z",
     "start_time": "2020-02-17T10:31:01.044565Z"
    }
   },
   "outputs": [],
   "source": [
    "som_dim = [16,16]\n",
    "latent_dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:16.678893Z",
     "start_time": "2020-02-17T10:29:16.670637Z"
    }
   },
   "outputs": [],
   "source": [
    "val_gen = batch_generator(data_train, data_val, endpoints_total_val, 300, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:38:43.859678Z",
     "start_time": "2020-02-17T10:38:38.982605Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File /home/matt/Training/dpsom/models/hyperopt_LSTM_20_16-16_2020-02-17_35a17/hyperopt_LSTM_20_16-16_2020-02-17_35a17.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-28035ae8c041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdata/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1458\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[0;32m-> 1460\u001b[0;31m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0m\u001b[1;32m   1461\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m                                                  **kwargs)[0]\n",
      "\u001b[0;32m~/.virtualenvs/bdata/lib/python3.8/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1475\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bdata/lib/python3.8/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    623\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File /home/matt/Training/dpsom/models/hyperopt_LSTM_20_16-16_2020-02-17_35a17/hyperopt_LSTM_20_16-16_2020-02-17_35a17.meta does not exist."
     ]
    }
   ],
   "source": [
    "num_batches = len(data_val) // 300\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "#tf.reset_default_graph()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    saver = tf.compat.v1.train.import_meta_graph(modelpath+\".meta\")\n",
    "    saver.restore(sess, modelpath)\n",
    "    graph = tf.get_default_graph()\n",
    "    k = graph.get_tensor_by_name(\"k/k:0\")\n",
    "    z_e = graph.get_tensor_by_name(\"z_e_sample/z_e:0\")\n",
    "    x = graph.get_tensor_by_name(\"inputs/x:0\")\n",
    "    is_training = graph.get_tensor_by_name(\"is_training/is_training:0\")\n",
    "    graph = tf.get_default_graph()\n",
    "    z_e_p = graph.get_tensor_by_name(\"prediction/next_state/input_lstm:0\")\n",
    "    q = graph.get_tensor_by_name(\"q/distribution/q:0\")\n",
    "    embeddings = graph.get_tensor_by_name(\"embeddings/embeddings:0\")\n",
    "    reconstruction = graph.get_tensor_by_name(\"reconstruction_e/x_hat:0\")\n",
    "    \n",
    "    print(\"Evaluation...\")\n",
    "    test_k_all = []\n",
    "    labels_val_all = []\n",
    "    z_e_all=[]\n",
    "    z_q_all = []\n",
    "    qq = []\n",
    "    for i in range(num_batches):\n",
    "            batch_data, batch_labels, ii = next(val_gen)\n",
    "            f_dic = {x: batch_data}\n",
    "            test_k_all.extend(sess.run(k, feed_dict=f_dic))\n",
    "            labels_val_all.extend(batch_labels)\n",
    "            z_q_all.extend(sess.run(q, feed_dict=f_dic))\n",
    "            qq.extend(sess.run(q, feed_dict=f_dic))\n",
    "            z_e_all.extend(sess.run(z_e, feed_dict=f_dic))\n",
    "    labels_val_all = np.array(labels_val_all)\n",
    "    k_all = np.array(test_k_all)\n",
    "    qq = np.array(qq)\n",
    "    labels_val_all = np.reshape(labels_val_all, (-1, labels_val_all.shape[-1]))\n",
    "    NMI_24 = metrics.normalized_mutual_info_score(labels_val_all[:, 3], k_all)\n",
    "    NMI_12 = metrics.normalized_mutual_info_score(labels_val_all[:, 2], k_all)\n",
    "    NMI_6 = metrics.normalized_mutual_info_score(labels_val_all[:, 1], k_all)\n",
    "    NMI_1 = metrics.normalized_mutual_info_score(labels_val_all[:, 0], k_all)\n",
    "    \n",
    "    embb = sess.run(embeddings, feed_dict={x: data_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:29:21.798617Z",
     "start_time": "2020-02-17T10:29:21.782730Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_val_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5f43af3f0f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels_12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels_6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_24\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhosp_disc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_val_all' is not defined"
     ]
    }
   ],
   "source": [
    "labels_12 = labels_val_all[:,2]\n",
    "labels_1 = labels_val_all[:,0]\n",
    "labels_6 = labels_val_all[:,1]\n",
    "labels_24 = labels_val_all[:,3]\n",
    "hosp_disc_1 = labels_val_all[:,4]\n",
    "hosp_disc_6 = labels_val_all[:,5]\n",
    "hosp_disc_12 = labels_val_all[:,6]\n",
    "hosp_disc_24 = labels_val_all[:,7]\n",
    "u_disc_1 = labels_val_all[:,8]\n",
    "u_disc_6 = labels_val_all[:,9]\n",
    "u_disc_12 = labels_val_all[:,10]\n",
    "u_disc_24 = labels_val_all[:, 11]\n",
    "labels_1 = labels_1.astype(int)\n",
    "labels_6 = labels_6.astype(int)\n",
    "labels_12 = labels_12.astype(int)\n",
    "labels_24 = labels_24.astype(int)\n",
    "hosp_disc_12 = hosp_disc_12.astype(int)\n",
    "hosp_disc_24 = hosp_disc_24.astype(int)\n",
    "hosp_disc_1 = hosp_disc_1.astype(int)\n",
    "hosp_disc_6 = hosp_disc_6.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moran Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:31:11.318739Z",
     "start_time": "2020-02-17T10:31:10.554833Z"
    }
   },
   "outputs": [],
   "source": [
    "sd = som_dim[0]*som_dim[1]\n",
    "mean = np.sum(labels_val_all[:, 0]) / len(labels_val_all[:, 0])\n",
    "ones = np.ones((len(np.reshape(k_all, (-1)))))\n",
    "clust_matr1 = np.zeros(som_dim[0]*som_dim[1])\n",
    "labels= labels_val_all[:, 0]\n",
    "for i in range(som_dim[0]*som_dim[1]):\n",
    "    dd = np.sum(ones[np.where(np.reshape(k_all, (-1))==i)])\n",
    "    if dd == 0:\n",
    "        s1 = 0\n",
    "    else:\n",
    "        s1 = np.sum(labels[np.where(np.reshape(k_all, (-1))==i)]) / np.sum(ones[np.where(np.reshape(k_all, (-1))==i)])\n",
    "    clust_matr1[i] = s1\n",
    "\n",
    "k = np.arange(0,sd)\n",
    "k1 = k // som_dim[0]\n",
    "k2 = k % som_dim[0]\n",
    "W = np.zeros((sd,sd))\n",
    "for i in range(sd):\n",
    "    for j in range(sd):\n",
    "        d1 = np.abs((k1[i] - k1[j]))\n",
    "        d2 = np.abs((k2[i] - k2[j]))\n",
    "        d1 = min(som_dim[0]-d1, d1)\n",
    "        d2 = min(som_dim[0]-d2, d2)\n",
    "        W[i,j] = np.exp(-(d1+d2))\n",
    "        if i==j:\n",
    "            W[i,j]=0\n",
    "M = 0\n",
    "N_n = 0\n",
    "for i in range(sd):\n",
    "    for j in range(sd):\n",
    "        M += (clust_matr1[i] -mean)*(clust_matr1[j] -mean)* W[i,j]\n",
    "for i in range(sd):\n",
    "    N_n += (clust_matr1[i]-mean)**2\n",
    "W_n = np.sum(W)\n",
    "I = M * sd / (N_n*W_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:31:11.984101Z",
     "start_time": "2020-02-17T10:31:11.979614Z"
    }
   },
   "outputs": [],
   "source": [
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APACHE score heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:33:38.063038Z",
     "start_time": "2020-02-17T10:33:37.657395Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = labels_1\n",
    "ones = np.ones((len(np.reshape(k_all, (-1)))))\n",
    "clust_matr1 = np.zeros(som_dim[0]*som_dim[1])\n",
    "clust_matr2 = np.zeros(som_dim[0]*som_dim[1])\n",
    "for i in range(som_dim[0]*som_dim[1]):\n",
    "    s1 = np.sum(labels[np.where(np.reshape(k_all, (-1))==i)]) / np.sum(ones[np.where(np.reshape(k_all, (-1))==i)])\n",
    "    clust_matr1[i] = s1\n",
    "clust_matr1 = np.reshape(clust_matr1, (som_dim[0],som_dim[1]))\n",
    "ax = sns.heatmap(clust_matr1, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:34:29.633148Z",
     "start_time": "2020-02-17T10:34:29.603935Z"
    }
   },
   "outputs": [],
   "source": [
    "T = []\n",
    "S = []\n",
    "for i in range(1000):\n",
    "    h = np.reshape(u_disc_1, (-1,72))\n",
    "    if np.max(h[i]) == 1:\n",
    "        T.append(i)\n",
    "    else:\n",
    "        S.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:35:43.671743Z",
     "start_time": "2020-02-17T10:35:43.663623Z"
    }
   },
   "outputs": [],
   "source": [
    "ind_r = np.random.random_integers(0, 50, 10)\n",
    "ind_s = np.random.random_integers(0, 50, 10)\n",
    "T = np.array(T)\n",
    "S = np.array(S)\n",
    "a = np.concatenate([S[ind_s], T[ind_r]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:34:59.544789Z",
     "start_time": "2020-02-17T10:34:59.537095Z"
    }
   },
   "outputs": [],
   "source": [
    "k_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:36:03.110527Z",
     "start_time": "2020-02-17T10:35:45.279049Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = labels_1\n",
    "it = 0\n",
    "k_all = np.reshape(k_all, (-1,72))\n",
    "fig, ax = plt.subplots(5, 4, figsize=(50,43)) \n",
    "ones = np.ones((len(np.reshape(k_all, (-1)))))\n",
    "clust_matr1 = np.zeros(som_dim[0]*som_dim[1])\n",
    "clust_matr2 = np.zeros(som_dim[0]*som_dim[1])\n",
    "for i in range(som_dim[0]*som_dim[1]):\n",
    "    s1 = np.sum(labels[np.where(np.reshape(k_all, (-1)) == i)]) / np.sum(ones[np.where(np.reshape(k_all, (-1))==i)])\n",
    "    clust_matr1[i] = s1\n",
    "clust_matr1 = np.reshape(clust_matr1, (som_dim[0],som_dim[1]))\n",
    "for t in a:\n",
    "    #fig, ax = plt.subplots(figsize=(10,7.5)) \n",
    "    if it > 9:\n",
    "        c = \"r\"\n",
    "        #print(t)\n",
    "    else:\n",
    "        c = \"g\"\n",
    "    cc = it % 4\n",
    "    rr = it // 4\n",
    "    g = sns.heatmap(clust_matr1, cmap=\"YlGnBu\",ax=ax[rr][cc])\n",
    "    k_1 = k_all[t] // som_dim[1]\n",
    "    k_2 = k_all[t] % som_dim[1]\n",
    "    ax[rr][cc].plot(k_2[:] + 0.5, k_1[:] + 0.5, color=c, linewidth=4)\n",
    "    ax[rr][cc].scatter(k_2[0] + 0.5, k_1[0] + 0.5, color=c, s=200, label='Start')\n",
    "    ax[rr][cc].scatter(k_2[1:-1] + 0.5, k_1[1:-1] + 0.5, color=c, linewidth=5, marker='.')\n",
    "    ax[rr][cc].scatter(k_2[-1] + 0.5, k_1[-1] + 0.5, color=c, s=500, linewidth=4, marker='x', label='End')\n",
    "    ax[rr][cc].legend(loc=2, prop={'size': 20})\n",
    "    it +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distribution over trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:39:00.594160Z",
     "start_time": "2020-02-17T10:39:00.586726Z"
    }
   },
   "outputs": [],
   "source": [
    "qq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:39:40.288817Z",
     "start_time": "2020-02-17T10:39:36.101842Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_q = np.reshape(qq, (-1, 72, som_dim[0]*som_dim[1])) \n",
    "i = np.random.randint(0, 50) #Randomly sampled patient\n",
    "it = 0\n",
    "fig, ax = plt.subplots(2, 3, figsize=(50,25))\n",
    "k_all = np.reshape(k_all, (-1,72))\n",
    "for t in [0, 17, 40, 57, 64, 71]:\n",
    "    cc = it % 3\n",
    "    rr = it // 3\n",
    "    k_1 = k_all[i] // som_dim[1]\n",
    "    k_2 = k_all[i] % som_dim[1]\n",
    "    c = \"black\"\n",
    "    g1 = sns.heatmap(np.reshape(prob_q[i, t], (som_dim[0],som_dim[1])), cmap='Reds', alpha=1,  ax=ax[rr][cc])\n",
    "    ax[rr][cc].plot(k_2[:] + 0.5, k_1[:] + 0.5, color=c, linewidth=6)\n",
    "    ax[rr][cc].scatter(k_2[0] + 0.5, k_1[0] + 0.5, color=c, s=800, label='Start')\n",
    "    ax[rr][cc].scatter(k_2[1:-1] + 0.5, k_1[1:-1] + 0.5, color=c, linewidth=10, marker='.')\n",
    "    ax[rr][cc].scatter(k_2[-1] + 0.5, k_1[-1] + 0.5, color=c, s=1200, linewidth=10, marker='x', label='End')\n",
    "    ax[rr][cc].legend(loc=2, prop={'size': 30})  \n",
    "    ax[rr][cc].set_title(\"Time-step = {}\".format(it*14), fontsize=40)\n",
    "    it +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T15:59:18.314040Z",
     "start_time": "2019-05-20T15:59:18.309526Z"
    }
   },
   "source": [
    "## Unrolling future time-steps and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:36:41.833543Z",
     "start_time": "2020-02-17T10:36:41.823206Z"
    }
   },
   "outputs": [],
   "source": [
    "def z_dist_flat(z_e, embeddings):\n",
    "    \"\"\"Computes the distances between the encodings and the embeddings.\"\"\"\n",
    "    emb = np.reshape(embeddings, (som_dim[0]*som_dim[1], -1))\n",
    "    z = np.reshape(z_e, (z_e.shape[0], 1, latent_dim))\n",
    "    z = np.tile(z, [1,som_dim[0]*som_dim[1], 1])\n",
    "    z_dist = np.square(z-emb)\n",
    "    z_dist_red = np.sum(z_dist, axis=-1)\n",
    "    return z_dist_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:36:41.983821Z",
     "start_time": "2020-02-17T10:36:41.978583Z"
    }
   },
   "outputs": [],
   "source": [
    "val_gen = batch_generator(data_train, data_val, endpoints_total_val, 300, mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:37:20.174178Z",
     "start_time": "2020-02-17T10:37:11.780587Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_batches = len(data_val) // 300\n",
    "latent_dim = 20\n",
    "num_pred = 6\n",
    "som = 16*16\n",
    "max_n_step = 72\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph(modelpath+\".meta\")\n",
    "    saver.restore(sess, modelpath)\n",
    "    graph = tf.get_default_graph()\n",
    "    k = graph.get_tensor_by_name(\"k/k:0\")\n",
    "    z_e = graph.get_tensor_by_name(\"z_e_sample/z_e:0\")\n",
    "    next_z_e = graph.get_tensor_by_name(\"prediction/next_z_e:0\")\n",
    "    x = graph.get_tensor_by_name(\"inputs/x:0\")\n",
    "    is_training = graph.get_tensor_by_name(\"is_training/is_training:0\")\n",
    "    graph = tf.get_default_graph()\n",
    "    init_1 = graph.get_tensor_by_name(\"prediction/next_state/init_state:0\")\n",
    "    z_e_p = graph.get_tensor_by_name(\"prediction/next_state/input_lstm:0\")\n",
    "    state1 = graph.get_tensor_by_name(\"prediction/next_state/next_state:0\")\n",
    "    q = graph.get_tensor_by_name(\"q/distribution/q:0\")\n",
    "    embeddings = graph.get_tensor_by_name(\"embeddings/embeddings:0\")\n",
    "    z_p = graph.get_tensor_by_name('reconstruction_e/decoder/z_e:0')\n",
    "    reconstruction = graph.get_tensor_by_name(\"reconstruction_e/x_hat:0\")\n",
    "    \n",
    "    print(\"Evaluation...\")\n",
    "    training_dic = {is_training: True, z_e_p: np.zeros((max_n_step * len(data_val), latent_dim)),\n",
    "                    init_1: np.zeros((2, batch_size, 100)), z_p: np.zeros((max_n_step * len(data_val), latent_dim))}\n",
    "    k_all = []\n",
    "    z_e_all=[]\n",
    "    z_q_all = []\n",
    "    qq = []\n",
    "    x_rec = []\n",
    "    for i in range(num_batches):\n",
    "            batch_data, batch_labels, ii = next(val_gen)\n",
    "            f_dic = {x: batch_data}\n",
    "            k_all.extend(sess.run(k, feed_dict=f_dic))\n",
    "            z_q_all.extend(sess.run(q, feed_dict=f_dic))\n",
    "            z_e_all.extend(sess.run(z_e, feed_dict=f_dic))\n",
    "            qq.extend(sess.run(q, feed_dict=f_dic))\n",
    "            f_dic.update(training_dic)\n",
    "            x_rec.extend(sess.run(reconstruction, feed_dict=f_dic))\n",
    "    z_e_all = np.array(z_e_all)\n",
    "    k_all = np.array(k_all)\n",
    "    qq = np.array(qq)\n",
    "    x_rec = np.array(x_rec)\n",
    "    z_e_all = z_e_all.reshape((-1, max_n_step, latent_dim))\n",
    "    k_all = k_all.reshape((-1, max_n_step))\n",
    "    \n",
    "    t = 72-num_pred\n",
    "    \n",
    "    embeddings = sess.run(embeddings, feed_dict={x: data_val[:, :t, :]})\n",
    "    embeddings = np.reshape(embeddings,(-1, latent_dim))\n",
    "    \n",
    "    z_e_o = z_e_all[:, :t, :]\n",
    "    k_o = k_all[:, :t]\n",
    "    k_eval=[]\n",
    "    next_z_e_o = []\n",
    "    state1_o =[]\n",
    "    for i in range(num_batches):\n",
    "        batch_data, batch_labels, ii = next(val_gen)\n",
    "        batch_data=batch_data[:, :t, :]\n",
    "        f_dic = {x: batch_data}\n",
    "        f_dic.update(training_dic)\n",
    "        next_z_e_o.extend(sess.run(next_z_e, feed_dict=f_dic))\n",
    "        if i == 0:\n",
    "            state1_o = sess.run(state1, feed_dict=f_dic)\n",
    "        else:\n",
    "            state1_o = np.concatenate([state1_o, sess.run(state1, feed_dict=f_dic)], axis=1)\n",
    "    next_z_e_o = np.array(next_z_e_o)\n",
    "    state1_o = np.array(state1_o)\n",
    "    \n",
    "    next_z_e_o_all = np.reshape(next_z_e_o[:, -1, :], (-1,1,latent_dim))\n",
    "    next_z_e_o = next_z_e_o[:, -1, :]\n",
    "    k_next = np.argmin(z_dist_flat(next_z_e_o, embeddings), axis=-1)\n",
    "    k_o = np.concatenate([k_o, np.expand_dims(k_next,1)], axis=1)\n",
    "    z_e_o = np.concatenate([z_e_o, np.expand_dims(next_z_e_o, 1)], axis=1)\n",
    "    f_dic = {x: np.zeros((len(data_val),1, 98)), is_training: False, z_e_p: np.zeros((1 * len(data_val), latent_dim)),\n",
    "             z_p: next_z_e_o, init_1: np.zeros((2, batch_size, 100))}\n",
    "    x_pred_hat = np.reshape(sess.run(reconstruction, feed_dict=f_dic), (-1, 1, 98))\n",
    "    \n",
    "    for i in range(num_pred-1):\n",
    "        print(i)\n",
    "        inp = data_val[:1500, (t + i), :]\n",
    "        f_dic = {x: np.reshape(inp, (inp.shape[0],1,inp.shape[1]))}\n",
    "        val_dic = {is_training: False, z_e_p: next_z_e_o, init_1: state1_o, z_p: np.zeros((max_n_step * len(inp), latent_dim))}\n",
    "        f_dic.update(val_dic)\n",
    "        next_z_e_o = sess.run(next_z_e, feed_dict=f_dic)\n",
    "        state1_o = sess.run(state1, feed_dict=f_dic)\n",
    "        next_z_e_o_all = np.concatenate([next_z_e_o_all, next_z_e_o], axis=1)\n",
    "        k_next = np.argmin(z_dist_flat(next_z_e_o, embeddings), axis=-1)\n",
    "        k_o = np.concatenate([k_o, np.expand_dims(k_next,1)], axis=1)\n",
    "        z_e_o = np.concatenate([z_e_o, next_z_e_o], axis=1)\n",
    "        next_z_e_o = np.reshape(next_z_e_o, (-1, latent_dim))\n",
    "        f_dic = {x: np.zeros((len(data_val),1, 98)), is_training: False, z_e_p: np.zeros((max_n_step * len(data_val), latent_dim)),\n",
    "             z_p: next_z_e_o, init_1: np.zeros((2, batch_size, 100))}\n",
    "        final_x = sess.run(reconstruction, feed_dict=f_dic)\n",
    "        x_pred_hat = np.concatenate([x_pred_hat, np.reshape(final_x, (-1, 1, 98))], axis = 1)\n",
    "    \n",
    "    f_dic = {x: np.zeros((1500,1, 98)), is_training: False, z_e_p: np.zeros((max_n_step * 1500, latent_dim)),\n",
    "             z_p: z_e_all[:, t-1, :], init_1: np.zeros((2, batch_size, 100))}\n",
    "    final_x = sess.run(reconstruction, feed_dict=f_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:37:20.241207Z",
     "start_time": "2020-02-17T10:37:20.196444Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(np.reshape(x_pred_hat, (-1, 98)), np.reshape(data_val[:1500, -num_pred:], (-1, 98)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of unrolled state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T10:37:20.332633Z",
     "start_time": "2020-02-17T10:37:20.246520Z"
    }
   },
   "outputs": [],
   "source": [
    "k_true = np.reshape(k_all[:, -num_pred:], (-1))\n",
    "k_pred = np.reshape(k_o[:, -num_pred:], (-1))\n",
    "tot = 0\n",
    "acc = 0\n",
    "for i in range(len(k_true)):\n",
    "    tot += 1\n",
    "    if k_true[i] == k_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc / tot\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
